{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive Bayes Classifier from scratch\n",
    "\n",
    "This notebook contains 5 functions that build the Naive Bayes Classifier from scratch that is without the use of Sklearn Library.\n",
    "\n",
    "- load_data() - loads the student.csv dataset into a more usable format\n",
    "- split_data() - splits the dataset using hold-out strategy\n",
    "- train() - trains the model\n",
    "- predict() - predicts the model\n",
    "- evaluate() - calculates the accuracy without the use of metrics\n",
    "- Last cell calculates precision, recall and f-1 scores without the use of metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should open a data file in csv, and transform it into a usable format\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file):\n",
    "    dataset = pd.read_csv(file)\n",
    "    return dataset \n",
    "\n",
    "dataset = load_data('student.csv')\n",
    "# storing all grades(class labels) separately\n",
    "grades = dataset['Grade'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should split a data set into a training set and hold-out test set\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def split_data(dataset):\n",
    "    # splitting dataset into training and test data of 70% and 30% respectively\n",
    "    train_data, test_data = train_test_split(dataset, test_size = 0.3)    \n",
    "    return train_data, test_data\n",
    "\n",
    "trains, test = split_data(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should build a supervised NB model\n",
    "\n",
    "def train(grades,trains):\n",
    "# calculating PRIORS\n",
    "    \n",
    "    prior_count={}\n",
    "    prior_prob={}\n",
    "    # prior_count consists of occurences of class labels\n",
    "    for grade in grades:\n",
    "        prior_count[grade]=np.sum(trains['Grade']==grade)\n",
    "    # prior_prob consists of probabilities of class labels\n",
    "    for value in prior_count:\n",
    "        prior_prob[value] = round(prior_count[value]/len(trains),2)\n",
    "\n",
    "# calculating LIKELIHOODS\n",
    "    attribute_count = {}\n",
    "    attribute_len = {}\n",
    "    for attribute in  trains:\n",
    "        attribute_count[attribute] = trains[attribute].unique()\n",
    "    for i in attribute_count:\n",
    "        attribute_len[i] = len(attribute_count[i])\n",
    "    #print(attribute_len)\n",
    "    \n",
    "    # conditional count consists of occurences of attribute values given class \n",
    "    conditional_count={}\n",
    "    #iterating through every attribute(column)\n",
    "    for attribute in trains:\n",
    "    #get unique values in column\n",
    "        unique_values = trains[attribute].unique()\n",
    "        value_dict={}\n",
    "    #iterating through every attribute value\n",
    "        for value in unique_values:\n",
    "            grade_dict={}\n",
    "     #iterating through all class labels \n",
    "            for grade in grades:\n",
    "    #counting the value associated with that grade and summing it up\n",
    "                grade_dict[grade]=((trains[attribute]==value) & (trains['Grade']==grade)).sum() \n",
    "            value_dict[value]=grade_dict\n",
    "        conditional_count[attribute]=value_dict\n",
    "\n",
    "    #conditional_count now consists of probabilities of each attribute value given class\n",
    "    for attr in conditional_count:\n",
    "        for val in conditional_count[attr]:\n",
    "            for grade in conditional_count[attr][val]:\n",
    "     # dividing each occurence of attribute value with total number of occurences of every class label\n",
    "                conditional_count[attr][val][grade] = round(((conditional_count[attr][val][grade]+1)/(prior_count[grade]+(attribute_len[attr]*1))),3)\n",
    "       \n",
    "     \n",
    "    return prior_prob,conditional_count\n",
    "\n",
    "prior,conditional =train(grades,trains)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function should predict the class for an instance or a set of instances, based on a trained model \n",
    "\n",
    "def predict(test,prior,conditional):\n",
    "    \n",
    "    # predicted_grades consists of preidcted class labels\n",
    "    predicted_grades=[]\n",
    "    # actual_grades consists of actual class labels\n",
    "    actual_grades=[]\n",
    "    \n",
    "    # iterating through every instance row by row in test data\n",
    "    for index,row in test.iterrows():\n",
    "    # setting max probability as -1 such that it's always true\n",
    "        maxscore=-1\n",
    "        maxgrade=''\n",
    "        scores={}\n",
    "    # iterating through every class label\n",
    "        for grade in grades:\n",
    "    # stored probability of prior\n",
    "            score=prior[grade]\n",
    "    # iterating through every attribute value\n",
    "            for attr in test:\n",
    "    # calculating using bayes rules i.e prior x liklihoods\n",
    "    # using conditional_count developed in previous function to calculate conditional probabilities\n",
    "    # attr = column name, row[attr]=value of row for that column\n",
    "                score*=conditional[attr][row[attr]][grade]  \n",
    "        \n",
    "    # scores consists of all probabilities calculated above\n",
    "            scores[grade]=score\n",
    "    # find grade with maximum probability\n",
    "            if score>maxscore:\n",
    "                maxscore=score\n",
    "                maxgrade=grade\n",
    "    # inserting predicted grades one by one\n",
    "        predicted_grades.append(maxgrade)\n",
    "    # inserting actual grades one by one\n",
    "        actual_grades.append(row['Grade'])\n",
    "    \n",
    "    return actual_grades,predicted_grades\n",
    "    \n",
    "            \n",
    "actual, predicted = predict(test, prior, conditional)\n",
    "#print(test)\n",
    "#print(conditional)\n",
    "#print(predicted)\n",
    "#print(actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94.35897435897435\n"
     ]
    }
   ],
   "source": [
    "# This function should evaluate a set of predictions in terms of accuracy\n",
    "\n",
    "def evaluate(actual,predicted,test):\n",
    "    \n",
    "    # initialising a counter\n",
    "    acc = 0;\n",
    "    \n",
    "    # iterating through preidcted grades and actual grades\n",
    "    for i in range(0,len(predicted)):\n",
    "    # if both are same, increment counter\n",
    "        if(actual[i]==predicted[i]):\n",
    "            acc+=1\n",
    "            \n",
    "    # divide counter by total number of instances in test data\n",
    "    return acc/len(test)*100\n",
    "\n",
    "accuracy = evaluate(actual,predicted,test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'B': 0.9473684210526315, 'D': 0.9666666666666667, 'F': 0.9565217391304348, 'C': 0.9795918367346939, 'A+': 0.75, 'A': 0.9047619047619048}\n",
      "{'B': 0.9230769230769231, 'D': 0.9830508474576272, 'F': 0.9166666666666666, 'C': 1.0, 'A+': 0.5, 'A': 1.0}\n",
      "{'B': 0.935064935064935, 'D': 0.9747899159663865, 'F': 0.9361702127659574, 'C': 0.9896907216494846, 'A+': 0.6, 'A': 0.9500000000000001}\n",
      "0.9174850947243888\n",
      "0.887132406200203\n",
      "0.9020534925751884\n"
     ]
    }
   ],
   "source": [
    "#calculating precision, recall and F-1 on test data\n",
    "import numpy as np\n",
    "\n",
    "matrix = {}\n",
    "\n",
    "# creating a confusion matrix initialised with 0\n",
    "for g1 in grades:\n",
    "    matrix[g1] = {}\n",
    "    for g2 in grades:\n",
    "        matrix[g1][g2] = 0\n",
    "# adding the occurences of grades in confusion matrix\n",
    "for i in range(len(test)):\n",
    "    matrix[predicted[i]][actual[i]] += 1\n",
    "\n",
    "# precision, recall and f-1  for every class\n",
    "precision = {}\n",
    "recall = {}\n",
    "f1 = {}\n",
    "for g1 in grades:\n",
    "    num = matrix[g1][g1]\n",
    "    den1 = 0\n",
    "    den2 = 0\n",
    "    for g2 in grades:\n",
    "        den1+=matrix[g1][g2]\n",
    "        den2+=matrix[g2][g1]\n",
    "    if(den1 == 0 & den2 == 0):\n",
    "        precision[g1] = 'NA'\n",
    "        recall[g1] = 'NA'\n",
    "    else:\n",
    "        precision[g1] = num/den1\n",
    "        recall[g1] = num/den2\n",
    "for g in grades:\n",
    "    if precision[g] == 'NA':\n",
    "        f1[g] = 'NA'\n",
    "    else:\n",
    "        f1[g] = ((2*precision[g]*recall[g])/(precision[g]+recall[g]))\n",
    "        \n",
    "# overall precison, recall and f-1\n",
    "overall_precision = 0\n",
    "overall_recall = 0\n",
    "c1 = 0\n",
    "c2 = 0\n",
    "for g1 in grades:\n",
    "    if(precision[g1] != 'NA'):\n",
    "        overall_precision += precision[g1]\n",
    "        c1+=1\n",
    "    if(recall[g1] != 'NA'):\n",
    "        overall_recall += recall[g1]\n",
    "        c2+=1\n",
    "\n",
    "overall_precision /= c1\n",
    "overall_recall /= c2\n",
    "overall_f1 = (2*overall_precision*overall_recall)/(overall_precision+overall_recall)\n",
    "        \n",
    "\n",
    "\n",
    "print(precision)\n",
    "print(recall)\n",
    "print(f1)\n",
    "print(overall_precision)\n",
    "print(overall_recall)\n",
    "print(overall_f1)\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
